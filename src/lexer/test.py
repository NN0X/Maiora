# This script was generated by ChatGPT based on user instructions.
# As such it is not under terms noted in NOTICE and LICENSING

import re
import subprocess
import sys
from pathlib import Path
import difflib

# Tolerant regex for token lines that include "Line: <n> | Pos: <p> | Data: <text>"
TOKEN_LINE_RE = re.compile(
    r'^\s*(?:Token\s+\d+\s*:)?\s*(?P<value>.*?)\s*\|\s*Line\s*:\s*(?P<line>\d+)\s*\|\s*Pos\s*:\s*(?P<pos>\d+)\s*\|\s*Data\s*:\s*(?P<data>.*)$'
)

# Fallback mapping when Data: is empty and only a numeric code or stub is present
FALLBACK_TOKEN_MAP = {
    '76': ' ',  # seen in example dumps representing a space
    'SPACE_STUB': ' ',
    'TOK_OP_SEMICOLON': ';',
    'TOK_OP_LBRACE': '{',
    'TOK_OP_RBRACE': '}',
    'TOK_OP_LPAREN': '(',
    'TOK_OP_RPAREN': ')',
}

def parse_token_line(line):
    m = TOKEN_LINE_RE.match(line)
    if not m:
        return None
    value = m.group('value').strip()
    line_no = int(m.group('line'))
    pos = int(m.group('pos'))
    data = m.group('data').rstrip()

    if data.strip() != '':
        token_text = data.strip()
    else:
        # fallback to visible token value or known numeric codes
        token_text = FALLBACK_TOKEN_MAP.get(value, value)
    return line_no, pos, token_text

def reconstruct_from_token_dump_text(token_dump_text):
    """
    Reconstruct source text from the token dump text (string).
    Returns (reconstructed_text, parsed_token_count).
    """
    lines = {}  # line_no -> dict(col -> token_text)
    parsed_count = 0

    for raw in token_dump_text.splitlines():
        parsed = parse_token_line(raw)
        if parsed is None:
            continue
        ln, pos, tok = parsed
        col = max(0, pos - 1)  # convert 1-indexed to 0-indexed
        if ln not in lines:
            lines[ln] = {}
        # avoid clobbering same column: find next free column if needed
        if col in lines[ln]:
            shift = 1
            while (col + shift) in lines[ln]:
                shift += 1
            col += shift
        lines[ln][col] = tok
        parsed_count += 1

    if parsed_count == 0:
        return "", 0

    max_line = max(lines.keys())
    out_lines = []
    for ln in range(1, max_line + 1):
        if ln not in lines:
            out_lines.append("")
            continue
        items = sorted(lines[ln].items(), key=lambda x: x[0])
        current_pos = 0
        pieces = []
        for col, text in items:
            if col > current_pos:
                pieces.append(" " * (col - current_pos))
            pieces.append(text)
            current_pos = col + len(text)
        out_lines.append("".join(pieces))
    reconstructed = "\n".join(out_lines)
    return reconstructed.rstrip("\n"), parsed_count

def call_main_capture(input_path: Path):
    """
    Run ./main input 1 and capture stdout. If stdout is empty, try ./main input token_dump.log.
    Returns tuple (token_dump_text, method, returncode, stderr_text).
    method is 'stdout' or 'file:<filename>' or 'none'.
    """
    proc = subprocess.run(["./main", str(input_path), "1"], capture_output=True, text=True)
    if proc.returncode != 0:
        # still check stdout if present
        if proc.stdout and proc.stdout.strip():
            return proc.stdout, "stdout", proc.returncode, proc.stderr or ""
        # try fallback anyway
    if proc.stdout and proc.stdout.strip():
        return proc.stdout, "stdout", proc.returncode, proc.stderr or ""

    # fallback: ask binary to write token_dump.log
    fallback_file = "token_dump.log"
    proc2 = subprocess.run(["./main", str(input_path), fallback_file], capture_output=True, text=True)
    # prefer reading the file if it exists
    if Path(fallback_file).exists():
        txt = Path(fallback_file).read_text(encoding="utf-8", errors="replace")
        return txt, f"file:{fallback_file}", proc2.returncode, proc2.stderr or ""
    # otherwise, return whatever proc2 gave (maybe stdout)
    if proc2.stdout and proc2.stdout.strip():
        return proc2.stdout, "stdout", proc2.returncode, proc2.stderr or ""
    return "", "none", proc2.returncode, proc2.stderr or ""

def unified_diff(a_text, b_text, a_name="original", b_name="reconstructed"):
    a_lines = a_text.rstrip("\n").splitlines()
    b_lines = b_text.rstrip("\n").splitlines()
    return list(difflib.unified_diff(a_lines, b_lines, fromfile=a_name, tofile=b_name, lineterm=""))

def main(argv):
    if len(argv) != 2:
        print(f"Usage: python {Path(argv[0]).name} <input_file>.mai")
        return 2

    input_path = Path(argv[1])
    if not input_path.exists():
        print(f"Error: input file {input_path} not found.")
        return 1

    token_dump_text, method, rc, stderr = call_main_capture(input_path)

    if rc != 0 and not token_dump_text.strip():
        print(f"Error: ./main returned exit code {rc} and produced no token output.")
        if stderr:
            print("stderr:", stderr.strip())
        return 1

    if not token_dump_text.strip():
        print("Error: no token output captured from ./main.")
        return 1

    reconstructed_text, parsed_count = reconstruct_from_token_dump_text(token_dump_text)

    if parsed_count == 0:
        print("Error: token dump parsed 0 token lines (format mismatch).")
        # for debugging, we will write the token dump for inspection
        Path("token_dump.log").write_text(token_dump_text, encoding="utf-8", errors="replace")
        print("Wrote token_dump.log for debugging.")
        return 1

    original_text = input_path.read_text(encoding="utf-8", errors="replace")

    # Compare ignoring a single trailing newline at EOF (like original script)
    if original_text.rstrip("\n") == reconstructed_text.rstrip("\n"):
        print("✅ Reconstructed source matches the input exactly. No dump files written.")
        return 0

    # Not identical -> write dumps for inspection
    token_path = Path("token_dump.log")
    reconstructed_path = Path("reconstructed.mai")
    token_path.write_text(token_dump_text, encoding="utf-8", errors="replace")
    reconstructed_path.write_text(reconstructed_text, encoding="utf-8", errors="replace")

    print("❌ Reconstructed source differs from input.")
    print(f"Saved token dump to: {token_path}")
    print(f"Saved reconstructed source to: {reconstructed_path}")
    print()
    diff_lines = unified_diff(original_text, reconstructed_text, a_name=str(input_path), b_name=str(reconstructed_path))
    if diff_lines:
        for line in diff_lines:
            print(line)
    else:
        print("(no textual diff produced; binary/encoding difference?)")

    return 0

if __name__ == "__main__":
    raise SystemExit(main(sys.argv))
